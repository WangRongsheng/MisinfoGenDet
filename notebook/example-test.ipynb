{"cells":[{"cell_type":"markdown","metadata":{"id":"i_NXzqRWEzla"},"source":["## 安装与环境设置"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18872,"status":"ok","timestamp":1690870195543,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"F4xdBifvyCxS","outputId":"0ce8f074-1cbe-4b74-ae60-236b97f40e59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain\n","  Downloading langchain-0.0.248-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n","Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n","  Downloading langsmith-0.0.16-py3-none-any.whl (29 kB)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n","Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n","  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n","Successfully installed dataclasses-json-0.5.14 langchain-0.0.248 langsmith-0.0.16 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n","Collecting openai\n","  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n","Installing collected packages: openai\n","Successfully installed openai-0.27.8\n"]}],"source":["!pip install langchain\n","!pip install openai\n","from langchain.llms import OpenAI\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain import FewShotPromptTemplate\n","from langchain.schema import(AIMessage,HumanMessage,SystemMessage)\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690870200671,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"DSot-U1bzREX"},"outputs":[],"source":["os.environ['OPENAI_API_KEY'] = 'sk-xxx'"]},{"cell_type":"markdown","metadata":{"id":"3AI48t-8uOeA"},"source":["## Few shot without question"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":407,"status":"ok","timestamp":1690870247183,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"b5DBuALgzaWN"},"outputs":[],"source":["def few_shot_cot_nqa(claim_text):\n","  examples = [\n","  {\"claim\": \"\"\" # The claim is that In 1959 , former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight division at the Pan American Games (held in Chicago, United States, from August 27 to September 7) in Chicago, United States, and the world amateur welterweight title in Mexico City. \"\"\",\n","  \"program\": \"\"\"\n","  def program ():\n","    fact_1 = Verify (\" Alfredo Cornejo Cuevas was born in June 6 , 1933. \")\n","    fact_2 = Verify (\" Alfredo Cornejo Cuevas won the gold medal in the welterweight division at the Pan American Games in 1959. \")\n","    fact_3 = Verify (\" The Pan American Games in 1959 was held in Chicago , United States , from August 27 to September 7.\")\n","    fact_4 = Verify (\" Alfredo Cornejo Cuevas won the world amateur welterweight title in Mexico City .\")\n","    label = Predict ( fact_1 and fact_2 and fact_3 and fact_4 )\n","  \"\"\"},\n","\n","  {\"claim\": \"\"\" # The claim is that The Footwork FA12 , which was intended to start the season , finally debuted at the San Marino Grand Prix , a Formula One motor race held at Imola on 28 April 1991.\"\"\",\n","  \"program\": \"\"\"\n","  def program ():\n","    fact_1 = Verify (\" The Footwork FA12 , which was intended to start the season .\")\n","    fact_2 = Verify (\" The Footwork FA12 finally debuted at the San Marino Grand Prix .\")\n","    fact_3 = Verify (\" The San Marino Grand Prix was a Formula One motor race held at Imola on 28 April 1991. \")\n","    label = Predict ( fact_1 and fact_2 and fact_3 )\n","  \"\"\"}]\n","\n","  example_template = \"\"\"claim: {claim}\n","program: {program}\"\"\"\n","  prefix = \"\"\"Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \"\"\"\n","  suffix =\"\"\"claim: {claim}\"\"\"\n","  example_prompt = PromptTemplate(input_variables = [\"claim\",\"program\"],template=example_template)\n","\n","  few_shot_prompt_template = FewShotPromptTemplate(\n","      examples = examples,\n","      example_prompt = example_prompt,\n","      prefix = prefix,\n","      suffix = suffix,\n","      input_variables = [\"claim\"],\n","      example_separator = \"\\n\\n\"\n","  )\n","\n","  max_tokens_chat = 2000\n","\n","  llm = ChatOpenAI(temperature=0,model_name='gpt-3.5-turbo',max_tokens=max_tokens_chat)\n","  from langchain.prompts.chat import (\n","      ChatPromptTemplate,\n","      SystemMessagePromptTemplate,\n","      HumanMessagePromptTemplate,\n","  )\n","  template = ''\n","  system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n","  human_template=\"{text}\"\n","  human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","  chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n","  chain = LLMChain(llm=llm, prompt=chat_prompt)\n","  answer = chain.run(text=few_shot_prompt_template.format(claim=claim_text))\n","\n","  return answer,few_shot_prompt_template.format(claim=claim_text)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3317,"status":"ok","timestamp":1690870255033,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"9ePlstPF1RGc"},"outputs":[],"source":["answer,input = few_shot_cot_nqa('# The claim is that Sumo wrestler Toyozakura Toshiaki committed match -fixing , ending his career in 2011 that started in 1989.')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1690870259027,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"MESl9DIX1a8R","outputId":"5b072e24-c441-4d8a-eddd-82c0da921d23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \n","\n","claim:  # The claim is that In 1959 , former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight division at the Pan American Games (held in Chicago, United States, from August 27 to September 7) in Chicago, United States, and the world amateur welterweight title in Mexico City. \n","program: \n","  def program ():\n","    fact_1 = Verify (\" Alfredo Cornejo Cuevas was born in June 6 , 1933. \")\n","    fact_2 = Verify (\" Alfredo Cornejo Cuevas won the gold medal in the welterweight division at the Pan American Games in 1959. \")\n","    fact_3 = Verify (\" The Pan American Games in 1959 was held in Chicago , United States , from August 27 to September 7.\")\n","    fact_4 = Verify (\" Alfredo Cornejo Cuevas won the world amateur welterweight title in Mexico City .\")\n","    label = Predict ( fact_1 and fact_2 and fact_3 and fact_4 )\n","  \n","\n","claim:  # The claim is that The Footwork FA12 , which was intended to start the season , finally debuted at the San Marino Grand Prix , a Formula One motor race held at Imola on 28 April 1991.\n","program: \n","  def program ():\n","    fact_1 = Verify (\" The Footwork FA12 , which was intended to start the season .\")\n","    fact_2 = Verify (\" The Footwork FA12 finally debuted at the San Marino Grand Prix .\")\n","    fact_3 = Verify (\" The San Marino Grand Prix was a Formula One motor race held at Imola on 28 April 1991. \")\n","    label = Predict ( fact_1 and fact_2 and fact_3 )\n","  \n","\n","claim: # The claim is that Sumo wrestler Toyozakura Toshiaki committed match -fixing , ending his career in 2011 that started in 1989.\n"," \n","program: \n","  def program ():\n","    fact_1 = Verify (\" Toyozakura Toshiaki started his sumo wrestling career in 1989. \")\n","    fact_2 = Verify (\" Toyozakura Toshiaki ended his sumo wrestling career in 2011. \")\n","    fact_3 = Verify (\" Toyozakura Toshiaki was involved in match-fixing. \")\n","    label = Predict ( fact_1 and fact_2 and fact_3 )\n"]}],"source":["print(input)\n","print(' ')\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"wnSuiVdUvMi5"},"source":["## Few shot with question"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":629,"status":"ok","timestamp":1690871419017,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"PZHfeYHrocJI"},"outputs":[],"source":["def few_shot_cot_qa(claim_text):\n","  examples = [\n","  {\"claim\": \"\"\" # The country that Fujairah College is located in had a 2013 population of 9.2 million until it was hit by the plague in 1483 when the population was halved. \"\"\",\n","  \"program\": \"\"\"\n","  def program ():\n","    answer_1 = Question(\"Which country is Fujairah College located in?\")\n","    fact_1 = Verify(\"[answer_1] had a 2013 population of 9.2 million.\")\n","    fact_2 = Verify(\"[answer_1] was hit by the plague in 1483.\")\n","    fact_3 = Verify(\"The population of [answer_1] was halved in 1483.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n","  \"\"\"},\n","\n","  {\"claim\": \"\"\" # The solicitor who won the show Back to Reality ahead of Maureen Rees and Craig Phillips is English. The solicitor that was a chair of Global Witness is also English. \"\"\",\n","  \"program\": \"\"\"\n","  def program ():\n","    answer_1 = Question(\"Which solicitor won the show Back to Reality ahead of Maureen Rees and Craig Phillips?\")\n","    answer_2 = Question(\"Which solicitor was a chair of Global Witness?\")\n","    fact_1 = Verify(\"[answer_1] is English.\")\n","    fact_2 = Verify(\"[answer_2] is English.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"}]\n","\n","  example_template = \"\"\"claim: {claim}\n","program: {program}\"\"\"\n","  prefix = \"\"\"Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \"\"\"\n","  suffix =\"\"\"claim: {claim}\"\"\"\n","  example_prompt = PromptTemplate(input_variables = [\"claim\",\"program\"],template=example_template)\n","\n","  few_shot_prompt_template = FewShotPromptTemplate(\n","      examples = examples,\n","      example_prompt = example_prompt,\n","      prefix = prefix,\n","      suffix = suffix,\n","      input_variables = [\"claim\"],\n","      example_separator = \"\\n\\n\"\n","  )\n","\n","  max_tokens_chat = 2000\n","\n","  llm = ChatOpenAI(temperature=0,model_name='gpt-3.5-turbo',max_tokens=max_tokens_chat)\n","  from langchain.prompts.chat import (\n","      ChatPromptTemplate,\n","      SystemMessagePromptTemplate,\n","      HumanMessagePromptTemplate,\n","  )\n","  template = ''\n","  system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n","  human_template=\"{text}\"\n","  human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","  chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n","  chain = LLMChain(llm=llm, prompt=chat_prompt)\n","  answer = chain.run(text=few_shot_prompt_template.format(claim=claim_text))\n","\n","  return answer,few_shot_prompt_template.format(claim=claim_text)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4259,"status":"ok","timestamp":1690871426774,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"3bukTfwBxuTg"},"outputs":[],"source":["answer,input = few_shot_cot_qa('# Anthony Burgess addressed the novelist and essayist, the author of Grimus, in a lengthy love letter. The author is of the same nationality as Raj Koothrappali.')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690871441522,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"2N3nsfhvyntV","outputId":"f7eae1ee-5642-4a91-93be-b2a7fb8b9a3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \n","\n","claim:  # The country that Fujairah College is located in had a 2013 population of 9.2 million until it was hit by the plague in 1483 when the population was halved. \n","program: \n","  def program ():\n","    answer_1 = Question(\"Which country is Fujairah College located in?\")\n","    fact_1 = Verify(\"[answer_1] had a 2013 population of 9.2 million.\")\n","    fact_2 = Verify(\"[answer_1] was hit by the plague in 1483.\")\n","    fact_3 = Verify(\"The population of [answer_1] was halved in 1483.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n","  \n","\n","claim:  # The solicitor who won the show Back to Reality ahead of Maureen Rees and Craig Phillips is English. The solicitor that was a chair of Global Witness is also English. \n","program: \n","  def program ():\n","    answer_1 = Question(\"Which solicitor won the show Back to Reality ahead of Maureen Rees and Craig Phillips?\")\n","    answer_2 = Question(\"Which solicitor was a chair of Global Witness?\")\n","    fact_1 = Verify(\"[answer_1] is English.\")\n","    fact_2 = Verify(\"[answer_2] is English.\")\n","    label = Predict(fact_1 and fact_2)\n","  \n","\n","claim: # Anthony Burgess addressed the novelist and essayist, the author of Grimus, in a lengthy love letter. The author is of the same nationality as Raj Koothrappali.\n"," \n","program: \n","  def program ():\n","    answer_1 = Question(\"Who is the novelist and essayist that Anthony Burgess addressed in a love letter?\")\n","    answer_2 = Question(\"What is the nationality of Raj Koothrappali?\")\n","    fact_1 = Verify(\"The author of Grimus is [answer_1].\")\n","    fact_2 = Verify(\"[answer_1] is of the same nationality as Raj Koothrappali.\")\n","    label = Predict(fact_1 and fact_2)\n"]}],"source":["# inference error\n","print(input)\n","print(' ')\n","print(answer)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3861,"status":"ok","timestamp":1690872945052,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"1J-0HB7K4Nsb","outputId":"5f9ddc24-2666-463f-825a-7e65930d580a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \n","\n","claim:  # The country that Fujairah College is located in had a 2013 population of 9.2 million until it was hit by the plague in 1483 when the population was halved. \n","program: \n","  def program ():\n","    answer_1 = Question(\"Which country is Fujairah College located in?\")\n","    fact_1 = Verify(\"[answer_1] had a 2013 population of 9.2 million.\")\n","    fact_2 = Verify(\"[answer_1] was hit by the plague in 1483.\")\n","    fact_3 = Verify(\"The population of [answer_1] was halved in 1483.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n","  \n","\n","claim:  # The solicitor who won the show Back to Reality ahead of Maureen Rees and Craig Phillips is English. The solicitor that was a chair of Global Witness is also English. \n","program: \n","  def program ():\n","    answer_1 = Question(\"Which solicitor won the show Back to Reality ahead of Maureen Rees and Craig Phillips?\")\n","    answer_2 = Question(\"Which solicitor was a chair of Global Witness?\")\n","    fact_1 = Verify(\"[answer_1] is English.\")\n","    fact_2 = Verify(\"[answer_2] is English.\")\n","    label = Predict(fact_1 and fact_2)\n","  \n","\n","claim: # The musician, who founded Morningwood with Max Green, is older than Max Green.\n"," \n","program: \n","  def program ():\n","    answer_1 = Question(\"Who founded Morningwood with Max Green?\")\n","    fact_1 = Verify(\"[answer_1] is a musician.\")\n","    answer_2 = Question(\"Who is Max Green?\")\n","    fact_2 = Verify(\"[answer_1] is older than [answer_2].\")\n","    label = Predict(fact_1 and fact_2)\n"]}],"source":["# inference error\n","answer,input = few_shot_cot_qa('# The musician, who founded Morningwood with Max Green, is older than Max Green.')\n","\n","print(input)\n","print(' ')\n","print(answer)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5918,"status":"ok","timestamp":1690873225319,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"2SZvI7A95JFc","outputId":"8fc21a1f-707a-4e69-fe1f-558213aea451"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \n","\n","claim:  # The country that Fujairah College is located in had a 2013 population of 9.2 million until it was hit by the plague in 1483 when the population was halved. \n","program: \n","  def program ():\n","    answer_1 = Question(\"Which country is Fujairah College located in?\")\n","    fact_1 = Verify(\"[answer_1] had a 2013 population of 9.2 million.\")\n","    fact_2 = Verify(\"[answer_1] was hit by the plague in 1483.\")\n","    fact_3 = Verify(\"The population of [answer_1] was halved in 1483.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n","  \n","\n","claim:  # The solicitor who won the show Back to Reality ahead of Maureen Rees and Craig Phillips is English. The solicitor that was a chair of Global Witness is also English. \n","program: \n","  def program ():\n","    answer_1 = Question(\"Which solicitor won the show Back to Reality ahead of Maureen Rees and Craig Phillips?\")\n","    answer_2 = Question(\"Which solicitor was a chair of Global Witness?\")\n","    fact_1 = Verify(\"[answer_1] is English.\")\n","    fact_2 = Verify(\"[answer_2] is English.\")\n","    label = Predict(fact_1 and fact_2)\n","  \n","\n","claim: # The record producer that produced the bluegrass album was born on 22 June, 1944. This album inspired a Tony award winning musical. This musical had a character that was originated by Carmen Cusack.\n"," \n","program: \n","  def program():\n","    answer_1 = Question(\"Who produced the bluegrass album?\")\n","    answer_2 = Question(\"When was the record producer born?\")\n","    answer_3 = Question(\"Which musical was inspired by the bluegrass album?\")\n","    answer_4 = Question(\"Who originated the character in the musical?\")\n","    fact_1 = Verify(\"[answer_1] was born on 22 June, 1944.\")\n","    fact_2 = Verify(\"The bluegrass album inspired a Tony award winning musical.\")\n","    fact_3 = Verify(\"The musical had a character that was originated by Carmen Cusack.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n"]}],"source":["# inference error\n","answer,input = few_shot_cot_qa('# The record producer that produced the bluegrass album was born on 22 June, 1944. This album inspired a Tony award winning musical. This musical had a character that was originated by Carmen Cusack.')\n","\n","print(input)\n","print(' ')\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"VQ1CCNBG67qc"},"source":["## Few shot with HOVER template"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":687,"status":"ok","timestamp":1690876514110,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"oL8yNJxD67DZ"},"outputs":[],"source":["def few_shot_cot_HOVER(claim_text):\n","  examples = [\n","    {\"claim\": \"\"\" # The claim is that Howard University Hospital and Providence Hospital are both located in Washington, D.C. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"Howard University Hospital is located in Washington, D.C.\")\n","        fact_2 = Verify(\"Providence Hospital is located in Washington, D.C.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that WWE Super Tuesday took place at an arena that currently goes by the name TD Garden. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which arena the WWE Super Tuesday took place?\")\n","        fact_1 = Verify(f\"[answer_1] currently goes by the name TD Garden.\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Talking Heads, an American rock band that was \"one of the most critically acclaimed bands of the 80's\" is featured in KSPN's AAA format. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"Talking Heads is an American rock band that was 'one of the most critically acclaimed bands of the 80's'.\")\n","        fact_2 = Verify(\"Talking Heads is featured in KSPN's AAA format.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that An IndyCar race driver drove a Formula 1 car designed by Peter McCool during the 2007 Formula One season. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which Formula 1 car was designed by Peter McCool during the 2007 Formula One season?\")\n","        fact_1 = Verify(f\"An IndyCar race driver drove the car [answer_1].\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Gina Bramhill was born in a village. The 2011 population of the area that includes this village was 167,446. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which village was Gina Bramhill born in?\")\n","        fact_1 = Verify(f\"The 2011 population of the area that includes [answer_1] was 167,446.\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Don Ashley Turlington graduated from Saint Joseph's College, a private Catholic liberal arts college in Standish. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"Saint Joseph's College is a private Catholic liberal arts college is located in Standish.\")\n","        fact_2 = Verify(f\"Don Ashley Turlington graduated from Saint Joseph's College.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Gael and Fitness are not published in the same country. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which country was Gael published in?\")\n","        answer_2 = Question(\"Which country was Fitness published in?\")\n","        fact_1 = Verify(f\"[answer_1] and [answer_2] are not the same country.\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Blackstar is the name of the album released by David Bowie that was recorded in secret. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"David Bowie released an album called Blackstar.\")\n","        fact_2 = Verify(\"David Bowie recorded an album in secret.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that In the 2004 Hockey film produced by a former major league baseball pitcher Kurt Russell played the USA coach. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which 2004 Hockey film was produced a former major league baseball pitcher?\")\n","        fact_1 = Verify(\"Kurt Russell played the USA coach in the film [answer_1].\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Along with the New York Islanders and the New York Rangers, the New Jersey Devils NFL franchise is popular in the New York metropolitan area. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"The New York Islanders and the New York Rangers are popular in the New York metropolitan area.\")\n","        fact_2 = Verify(\"The New Jersey Devils NFL franchise is popular in the New York metropolitan area.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Jack McFarland is the best known role of the host of the 64th Annual Tony Awards. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Who is the host of the 64th Annual Tony Awards?\")\n","        fact_1 = Verify(f\\\"Jack McFarland is the best known role of [answer_1].)\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that The song recorded by Fergie that was produced by Polow da Don and was followed by Life Goes On was M.I.L.F.$. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"M.I.L.F.$ was recorded by Fergie that was produced by Polow da Don.\")\n","        fact_2 = Verify(\"M.I.L.F.$ was was followed by Life Goes On.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Eatza Pizza and Your Pie were not founded in the same state. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which state was Eatza Pizza founded in?\")\n","        answer_2 = Question(\"Which state was Your Pie founded in?\")\n","        fact_1 = Verify(f\"[answer_1] and [answer_2] are not the same state.\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Gregg Rolie and Rob Tyner, are not a keyboardist. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"Gregg Rolie is not a keyboardist.\")\n","        fact_2 = Verify(\"Rob Tyner is not a keyboardist.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Maria Esther Andion Bueno, not Jimmy Connors, is the player that is from Brazil. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"Maria Esther Andion Bueno is from Brazil.\")\n","        fact_2 = Verify(\"Jimmy Connors is not from Brazil.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Vladimir Igorevich Arnold died after Georg Cantor. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"When did Vladimir Igorevich Arnold die?\")\n","        answer_2 = Question(\"When did Georg Cantor die?\")\n","        fact_1 = Verify(f\"[answer_1] is after [answer_2].\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Barton Mine was halted by a natural disaster not Camlaren Mine. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        fact_1 = Verify(\"Barton Mine was halted by a natural disaster.\")\n","        fact_2 = Verify(\"Camlaren Mine was not halted by a natural disaster.\")\n","        label = Predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that John O'Hara and Rabindranath Tagore are not the same nationality. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"What is the nationality of John O'Hara?\")\n","        answer_2 = Question(\"What is the nationality of Rabindranath Tagore?\")\n","        fact_1 = Verify(f\"[answer_1] and [answer_2] are not the same nationality.\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that Thomas Loren Friedman has won more Pulitzer Prizes than Colson Whitehead. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"How many Pulitzer Prizes has Thomas Loren Friedman won?\")\n","        answer_2 = Question(\"How many Pulitzer Prizes has Colson Whitehead won?\")\n","        fact_1 = Verify(f\"[answer_1] is more than [answer_2].\")\n","        label = Predict(fact_1)\n","    \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that The model of car Trevor Bayne drives was introduced for model year 2006. The Rookie of The Year in the 1997 CART season drives it in the NASCAR Sprint Cup. \"\"\",\n","    \"program\": \"\"\"\n","    def program():\n","        answer_1 = Question(\"Which model of car is drived by Trevor Bayne?\")\n","        fact_1 = Verify(f\"[answer_1] was introduced for model year 2006.\")\n","        answer_2 = Question(\"Who is the Rookie of The Year in the 1997 CART season?\")\n","        fact_2 = Verify(f\"[answer_2] drives the model of car Trevor Bayne drives in the NASCAR Sprint Cup.\")\n","        label = predict(fact_1 and fact_2)\n","    \"\"\"},\n","\n","]\n","\n","  example_template = \"\"\"claim: {claim}\n","program: {program}\"\"\"\n","  prefix = \"\"\"Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \"\"\"\n","  suffix =\"\"\"claim: {claim}\"\"\"\n","  example_prompt = PromptTemplate(input_variables = [\"claim\",\"program\"],template=example_template)\n","\n","  few_shot_prompt_template = FewShotPromptTemplate(\n","      examples = examples,\n","      example_prompt = example_prompt,\n","      prefix = prefix,\n","      suffix = suffix,\n","      input_variables = [\"claim\"],\n","      example_separator = \"\\n\\n\"\n","  )\n","\n","  max_tokens_chat = 2000\n","  temperature_set = 0.7\n","\n","  llm = ChatOpenAI(temperature=temperature_set,model_name='gpt-3.5-turbo',max_tokens=max_tokens_chat)\n","  from langchain.prompts.chat import (\n","      ChatPromptTemplate,\n","      SystemMessagePromptTemplate,\n","      HumanMessagePromptTemplate,\n","  )\n","  template = ''\n","  system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n","  human_template=\"{text}\"\n","  human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","  chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n","  chain = LLMChain(llm=llm, prompt=chat_prompt)\n","  answer = chain.run(text=few_shot_prompt_template.format(claim=claim_text))\n","\n","  return answer,few_shot_prompt_template.format(claim=claim_text)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3633,"status":"ok","timestamp":1690875714806,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"U8gfcDkyCQs_","outputId":"682e9583-4d56-4bed-817f-57709325388b"},"outputs":[{"name":"stdout","output_type":"stream","text":["program: \n","    def program():\n","        answer_1 = Question(\"Who founded Morningwood with Max Green?\")\n","        answer_2 = Question(\"Who is older, the musician or Max Green?\")\n","        fact_1 = Verify(f\"The musician who founded Morningwood with [answer_1] is older than Max Green.\")\n","        label = Predict(fact_1)\n"]}],"source":["# inference error\n","answer,input = few_shot_cot_HOVER('# The musician, who founded Morningwood with Max Green, is older than Max Green.')\n","\n","# print(input)\n","# print(' ')\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"nydOi8TX9JJ6"},"source":["## Few shot with FEVEROUS-S template"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":598,"status":"ok","timestamp":1690876635298,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"pUamKPeg9NVU"},"outputs":[],"source":["def few_shot_cot_FS(claim_text):\n","  examples = [\n","  {\"claim\": \"\"\" # The claim is that In 1959 , former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight division at the Pan American Games (held in Chicago, United States, from August 27 to September 7) in Chicago, United States, and the world amateur welterweight title in Mexico City. \"\"\",\n","  \"program\": \"\"\"\n","  def program ():\n","    fact_1 = Verify (\" Alfredo Cornejo Cuevas was born in June 6 , 1933. \")\n","    fact_2 = Verify (\" Alfredo Cornejo Cuevas won the gold medal in the welterweight division at the Pan American Games in 1959. \")\n","    fact_3 = Verify (\" The Pan American Games in 1959 was held in Chicago , United States , from August 27 to September 7.\")\n","    fact_4 = Verify (\" Alfredo Cornejo Cuevas won the world amateur welterweight title in Mexico City .\")\n","    label = Predict ( fact_1 and fact_2 and fact_3 and fact_4 )\n","  \"\"\"},\n","\n","  {\"claim\": \"\"\" # The claim is that The Footwork FA12 , which was intended to start the season , finally debuted at the San Marino Grand Prix , a Formula One motor race held at Imola on 28 April 1991.\n","  \"\"\",\n","  \"program\": \"\"\"\n","  def program ():\n","    fact_1 = Verify (\" The Footwork FA12 , which was intended to start the season .\")\n","    fact_2 = Verify (\" The Footwork FA12 finally debuted at the San Marino Grand Prix .\")\n","    fact_3 = Verify (\" The San Marino Grand Prix was a Formula One motor race held at Imola on 28 April 1991. \")\n","    label = Predict ( fact_1 and fact_2 and fact_3 )\n","  \"\"\"},\n","\n","    {\"claim\": \"\"\" # The claim is that SkyHigh Mount Dandenong (formerly Mount Dandenong Observatory) is a restaurant located on top of Mount Dandenong, Victoria, Australia. \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"SkyHigh Mount Dandenong is a restaurant located on top of Mount Dandenong, Victoria, Australia.\")\n","    fact_2 = Verify(\"SkyHigh Mount Dandenong is formerly known as Mount Dandenong Observatory.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"},\n","\n","      {\"claim\": \"\"\"# The claim is that Before the first Europeans arrived or copra companies leased it, Maupihaa was home to Inca's in ancient times. \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Maupihaa was home to Inca's in ancient times.\")\n","    fact_2 = Verify(\"Maupihaa was home to Inca's before the first Europeans arrived or copra companies leased it.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"},\n","\n","      {\"claim\": \"\"\"# The claim is that Shulin, a 33.1288 km (12.7911 sq mi) land located in New Taipei City, China, a country in East Asia, has a total population of 183,946 in December 2018.\"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Shulin is a 33.1288 km (12.7911 sq mi) land located in New Taipei City, China.\")\n","    fact_2 = Verify(\"Shulin has a total population of 183,946 in December 2018.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"},\n","\n","      {\"claim\": \"\"\"\n","      # The claim is that Sumo wrestler Toyozakura Toshiaki committed match-fixing, ending his career in 2011 that started in 1989.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Toyozakura Toshiaki ended his career in 2011 that started in 1989.\")\n","    fact_2 = Verify(\"Toyozakura Toshiaki is a Sumo wrestler.\")\n","    fact_3 = Verify(\"Toyozakura Toshiaki committed match-fixing.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n","  \"\"\"},\n","\n","        {\"claim\": \"\"\"\n","      # The claim is that In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight division at the Pan American Games (held in Chicago, United States, from August 27 to September 7) in Chicago, United States, and the world amateur welterweight title in Mexico City.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Alfredo Cornejo Cuevas is a former Chilean boxer.\")\n","    fact_2 = Verify(\"Alfredo Cornejo won the gold medal in the welterweight division at the Pan American Games.\")\n","    fact_3 = Verify(\"The Pan American Games was held in Chicago, United States, from August 27 to September 7.\")\n","    fact_4 = Verify(\"Alfredo Cornejo won the world amateur welterweight title in Mexico City.\")\n","    label = Predict(fact_1 and fact_2 and fact_3 and fact_4)\n","  \"\"\"},\n","        {\"claim\": \"\"\"\n","      # The claim is that Adductor hiatus is associated with nine structures, seven of which enter and leave through hiatus.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Adductor hiatus is associated with nine structures.\")\n","    fact_2 = Verify(\"Seven of the nine structures associated with Adductor hiatus enter and leave through hiatus.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"},\n","          {\"claim\": \"\"\"\n","      # The claim is that Ifor Bowen Lloyd was educated at Winchester (an independent boarding school for boys in the British public school tradition) and Exeter College, Oxford where he was a member of the Library Committee of the Oxford Union Society, as well as, received a BA in Modern History in 1924.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Ifor Bowen Lloyd was educated at Winchester and Exeter College, Oxford.\")\n","    fact_2 = Verify(\"Winchester is an independent boarding school for boys in the British public school tradition.\")\n","    fact_3 = Verify(\"While at Oxford, Ifor Bowen Lloyd was a member of the Library Committee of the Oxford Union Society.\")\n","    fact_4 = Verify(\"Ifor Bowen Lloyd received a BA in Modern History in 1924 at Oxford.\")\n","    label = Predict(fact_1 and fact_2 and fact_3 and fact_4)\n","  \"\"\"},\n","          {\"claim\": \"\"\"\n","      # The claim is that In the 2001 Stanley Cup playoffs Eastern Conference Semifinals Devils' Elias scored and Maple Leafs' left Devils player Scott Neidermayer hurt.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"In the 2001 Stanley Cup playoffs Eastern Conference Semifinals Devils' Elias scored.\")\n","    fact_2 = Verify(\"Maple Leafs' left Devils player Scott Neidermayer hurt.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"},\n","          {\"claim\": \"\"\"\n","      # The claim is that Teldenia helena is a moth first described in 1967 by Wilkinson.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"Teldenia helena is a moth.\")\n","    fact_2 = Verify(\"Teldenia helena was first described by Wilkinson in 1967.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"},\n","          {\"claim\": \"\"\"\n","      # The claim is that Born December 30, 1974, William Frick was a dark horse candidate in the Maryland House of Delegates appointment process.\n","      \"\"\",\n","  \"program\": \"\"\"\n","def program():\n","    fact_1 = Verify(\"William Frick was born in December 30, 1974.\")\n","    fact_2 = Verify(\"William Frick was a dark horse candidate in the Maryland House of Delegates appointment process.\")\n","    label = Predict(fact_1 and fact_2)\n","  \"\"\"}\n","  ]\n","\n","  example_template = \"\"\"claim: {claim}\n","program: {program}\"\"\"\n","  prefix = \"\"\"Generate a python-like program that describes the reasoning steps required to verify the claim step-by-step . You can call three functions in the program : 1. Question () to answer a question ; 2. Verify () to verify a simple claim ; 3.Predict () to predict the veracity label . \"\"\"\n","  suffix =\"\"\"claim: {claim}\"\"\"\n","  example_prompt = PromptTemplate(input_variables = [\"claim\",\"program\"],template=example_template)\n","\n","  few_shot_prompt_template = FewShotPromptTemplate(\n","      examples = examples,\n","      example_prompt = example_prompt,\n","      prefix = prefix,\n","      suffix = suffix,\n","      input_variables = [\"claim\"],\n","      example_separator = \"\\n\\n\"\n","  )\n","\n","  max_tokens_chat = 2000\n","  temperature_set = 0.7\n","\n","  llm = ChatOpenAI(temperature=temperature_set,model_name='gpt-3.5-turbo',max_tokens=max_tokens_chat)\n","  from langchain.prompts.chat import (\n","      ChatPromptTemplate,\n","      SystemMessagePromptTemplate,\n","      HumanMessagePromptTemplate,\n","  )\n","  template = ''\n","  system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n","  human_template=\"{text}\"\n","  human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","  chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n","  chain = LLMChain(llm=llm, prompt=chat_prompt)\n","  answer = chain.run(text=few_shot_prompt_template.format(claim=claim_text))\n","\n","  return answer,few_shot_prompt_template.format(claim=claim_text)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2547,"status":"ok","timestamp":1690874375461,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"dPKG2-JJ9SmR","outputId":"0c29cd5a-0603-47d1-fc1c-acf942069daf"},"outputs":[{"name":"stdout","output_type":"stream","text":["program: \n","def program():\n","    fact_1 = Verify(\"The musician founded Morningwood with Max Green.\")\n","    fact_2 = Verify(\"The musician is older than Max Green.\")\n","    label = Predict(fact_1 and fact_2)\n"]}],"source":["# inference error\n","answer,input = few_shot_cot_FS('# The musician, who founded Morningwood with Max Green, is older than Max Green.')\n","\n","# print(input)\n","# print(' ')\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"l99D_iCMHuCE"},"source":["## example from paper"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11014,"status":"ok","timestamp":1690877835137,"user":{"displayName":"王荣胜","userId":"15711791578430764716"},"user_tz":-480},"id":"5qD6O5OVGf2x","outputId":"4a748db0-adc1-44f4-f8eb-5ef85ca2e2a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["program: \n","    def program():\n","        answer_1 = Question(\"Which critically acclaimed film did Buddy Baker score in 1975?\")\n","        fact_1 = Verify(f\"The film [answer_1] is a Walt Disney film.\")\n","        answer_2 = Question(\"Which film featured Bruce M. Fischer as Mr. Coogar?\")\n","        fact_2 = Verify(f\"The film produced first before [answer_2] is the film Buddy Baker scored in 1975.\")\n","        label = Predict(fact_1 and fact_2)\n","\n","\n","program: \n","def program():\n","    fact_1 = Verify(\"The critically acclaimed film was scored by Buddy Baker in 1975.\")\n","    fact_2 = Verify(\"The critically acclaimed film is a Walt Disney film.\")\n","    fact_3 = Verify(\"The film featuring Bruce M. Fischer as Mr. Coogar was produced after the critically acclaimed film.\")\n","    label = Predict(fact_1 and fact_2 and fact_3)\n"]}],"source":["sen = '# The critically acclaimed film, that Buddy Baker scored in 1975, is a Walt Disney film. It was produced first before the film that featured Bruce M. Fischer as Mr. Coogar. '\n","\n","# HOVER\n","answer_hr,input = few_shot_cot_HOVER(sen)\n","# FEVEROUS-S\n","answer_fs,input = few_shot_cot_FS(sen)\n","\n","print(answer_hr)\n","print('\\n')\n","print(answer_fs)"]},{"cell_type":"markdown","metadata":{"id":"cim6uE89x8mg"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
